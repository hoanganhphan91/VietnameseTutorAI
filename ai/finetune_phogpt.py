#!/usr/bin/env python3
"""
Fine-tune PhoGPT-4B for Vietnamese Language Tutoring
"""
import json
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer
)
import torch

# Training data for Vietnamese tutoring
TRAINING_DATA = [
    {
        "prompt": "Há»c viÃªn: Xin chÃ o, tÃ´i muá»‘n há»c tiáº¿ng Viá»‡t",
        "response": "Xin chÃ o! TÃ´i ráº¥t vui Ä‘Æ°á»£c giÃºp báº¡n há»c tiáº¿ng Viá»‡t. HÃ£y báº¯t Ä‘áº§u vá»›i nhá»¯ng cÃ¢u chÃ o há»i cÆ¡ báº£n: 'Xin chÃ o' (sin chow) = Hello, 'Cáº£m Æ¡n' (gahm uhn) = Thank you. Báº¡n Ä‘Ã£ biáº¿t tiáº¿ng Viá»‡t Ä‘áº¿n má»©c nÃ o rá»“i?"
    },
    {
        "prompt": "Há»c viÃªn: Dáº¡y tÃ´i thanh Ä‘iá»‡u tiáº¿ng Viá»‡t",
        "response": "Tiáº¿ng Viá»‡t cÃ³ 6 thanh Ä‘iá»‡u quan trá»ng:\n1. Ngang (a) - khÃ´ng dáº¥u\n2. Huyá»n (Ã ) - dáº¥u huyá»n\n3. Sáº¯c (Ã¡) - dáº¥u sáº¯c\n4. Há»i (áº£) - dáº¥u há»i\n5. NgÃ£ (Ã£) - dáº¥u ngÃ£\n6. Náº·ng (áº¡) - dáº¥u náº·ng\n\nVÃ­ dá»¥ vá»›i tá»« 'ma': ma (ma quá»·), mÃ  (but), mÃ¡ (mother), máº£ (grave), mÃ£ (code), máº¡ (rice seedling). Má»—i thanh cÃ³ nghÄ©a khÃ¡c nhau!"
    },
    {
        "prompt": "Há»c viÃªn: TÃ´i khÃ´ng phÃ¡t Ã¢m Ä‘Æ°á»£c",
        "response": "Äá»«ng lo láº¯ng! PhÃ¡t Ã¢m tiáº¿ng Viá»‡t cáº§n thá»i gian luyá»‡n táº­p. Máº¹o hay:\n1. Nghe vÃ  nháº¯c láº¡i tá»«ng thanh Ä‘iá»‡u\n2. Báº¯t Ä‘áº§u vá»›i tá»« Ä‘Æ¡n giáº£n: mÃ¡, ba, cÆ¡m\n3. Luyá»‡n táº­p hÃ ng ngÃ y 10-15 phÃºt\n4. Ghi Ã¢m giá»ng mÃ¬nh Ä‘á»ƒ so sÃ¡nh\n\nHÃ£y thá»­ phÃ¡t Ã¢m 'mÃ¡' (mother) vá»›i giá»ng lÃªn cao nhÃ©!"
    }
]

def create_training_dataset():
    """Create training dataset"""
    formatted_data = []
    for item in TRAINING_DATA:
        text = f"### {item['prompt']}\n### Gia sÆ°: {item['response']}"
        formatted_data.append({"text": text})
    
    return Dataset.from_list(formatted_data)

def fine_tune_phogpt():
    """Fine-tune PhoGPT for Vietnamese tutoring"""
    print("ğŸš€ Starting PhoGPT Fine-tuning for Vietnamese Tutoring...")
    
    # Load model and tokenizer
    model_name = "vinai/PhoGPT-4B-Chat"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name, 
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    
    # Prepare dataset
    dataset = create_training_dataset()
    
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./phogpt_finetuned",
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=2e-5,
        warmup_steps=100,
        logging_steps=10,
        save_strategy="epoch",
        evaluation_strategy="no",
        fp16=torch.cuda.is_available(),
        push_to_hub=False,
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )
    
    # Start training
    print("ğŸ“š Training started...")
    trainer.train()
    
    # Save model
    trainer.save_model()
    tokenizer.save_pretrained("./phogpt_finetuned")
    
    print("âœ… Fine-tuning completed!")
    print("ğŸ“ Model saved to: ./phogpt_finetuned")

if __name__ == "__main__":
    fine_tune_phogpt()