#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Vietnamese Teacher Model - Kiá»ƒm tra model Ä‘Ã£ train chÆ°a
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

def test_trained_model():
    print("ğŸ” KIá»‚M TRA MODEL ÄÃƒ TRAIN")
    print("=" * 50)
    
    trained_path = "./vietnamese_teacher_trained"
    
    # Kiá»ƒm tra thÆ° má»¥c model
    if not os.path.exists(trained_path):
        print("âŒ CHÆ¯A CÃ“ MODEL TRAIN - ThÆ° má»¥c khÃ´ng tá»“n táº¡i")
        return False
    
    files = os.listdir(trained_path)
    print(f"ğŸ“ Files trong thÆ° má»¥c: {len(files)} files")
    
    required_files = ['model.safetensors', 'config.json', 'tokenizer_config.json']
    missing_files = [f for f in required_files if f not in files]
    
    if missing_files:
        print(f"âŒ THIáº¾U FILES QUAN TRá»ŒNG: {missing_files}")
        return False
    
    print("âœ… Táº¥t cáº£ files cáº§n thiáº¿t Ä‘á»u cÃ³")
    
    # Load vÃ  test model
    try:
        print("\nğŸ“¦ Loading model...")
        tokenizer = AutoTokenizer.from_pretrained(trained_path, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(trained_path, torch_dtype=torch.float32)
        
        print("âœ… Model load thÃ nh cÃ´ng!")
        
        # Test generation vá»›i cÃ¢u há»i tá»« training data
        test_question = "6 thanh Ä‘iá»‡u lÃ  nhá»¯ng thanh nÃ o áº¡?"
        print(f"\nğŸ§ª TEST CÃ‚U Há»I: {test_question}")
        
        prompt = f"<|startoftext|>Há»c sinh: {test_question}\nGiÃ¡o viÃªn:"
        inputs = tokenizer.encode(prompt, return_tensors='pt')
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.size(1) + 100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if "GiÃ¡o viÃªn:" in response:
            teacher_response = response.split("GiÃ¡o viÃªn:")[-1].strip()
            print(f"ğŸ¤– AI PHáº¢N Há»’I: {teacher_response}")
            
            # Kiá»ƒm tra xem cÃ³ há»c Ä‘Æ°á»£c tá»« training data khÃ´ng
            keywords = ["thanh ngang", "thanh huyá»n", "thanh sáº¯c", "6 thanh", "bÃ ", "ba"]
            learned = any(keyword in teacher_response.lower() for keyword in keywords)
            
            if learned:
                print("âœ… MODEL ÄÃƒ Há»ŒC Tá»ª TRAINING DATA!")
                return True
            else:
                print("âš ï¸  Model chÆ°a há»c tá»‘t tá»« training data")
                return False
        else:
            print("âŒ Model khÃ´ng táº¡o Ä‘Æ°á»£c response Ä‘Ãºng format")
            return False
            
    except Exception as e:
        print(f"âŒ Lá»–I KHI TEST MODEL: {e}")
        return False

if __name__ == '__main__':
    result = test_trained_model()
    print("\n" + "=" * 50)
    if result:
        print("ğŸ‰ Káº¾T LUáº¬N: MODEL ÄÃƒ ÄÆ¯á»¢C TRAIN THÃ€NH CÃ”NG!")
    else:
        print("ğŸ’¥ Káº¾T LUáº¬N: MODEL CHÆ¯A TRAIN HOáº¶C TRAIN THáº¤T Báº I!")