#!/usr/bin/env python3
"""
PhoGPT Fine-tuning Script for Vietnamese Tutoring
"""
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import os

# Training configuration
MODEL_NAME = "vinai/PhoGPT-4B-Chat"
OUTPUT_DIR = "./phogpt-vietnamese-tutor"
CACHE_DIR = "./.cache"

# Sample training data for Vietnamese tutoring
TRAINING_DATA = [
    {
        "conversation": [
            {"role": "student", "content": "Xin chÃ o, tÃ´i muá»‘n há»c tiáº¿ng Viá»‡t"},
            {"role": "tutor", "content": "Xin chÃ o! TÃ´i ráº¥t vui Ä‘Æ°á»£c dáº¡y báº¡n tiáº¿ng Viá»‡t. ChÃºng ta sáº½ báº¯t Ä‘áº§u tá»« nhá»¯ng Ä‘iá»u cÆ¡ báº£n nháº¥t. TrÆ°á»›c tiÃªn, hÃ£y há»c cÃ¡ch chÃ o há»i:\n\nâ€¢ 'Xin chÃ o' - Hello (formal)\nâ€¢ 'ChÃ o báº¡n' - Hi friend\nâ€¢ 'ChÃºc buá»•i sÃ¡ng tá»‘t lÃ nh' - Good morning\n\nBáº¡n cÃ³ muá»‘n thá»±c hÃ nh phÃ¡t Ã¢m khÃ´ng?"}
        ]
    },
    {
        "conversation": [
            {"role": "student", "content": "LÃ m sao phÃ¡t Ã¢m 'xin chÃ o' cho Ä‘Ãºng?"},
            {"role": "tutor", "content": "Tuyá»‡t! HÃ£y phÃ¢n tÃ­ch tá»«ng tá»«:\n\nğŸ”¤ **'Xin' [sin]:**\n- X = 's' sound\n- i = 'ee' sound  \n- n = 'n' sound\n- Thanh ngang (khÃ´ng lÃªn xuá»‘ng)\n\nğŸ”¤ **'ChÃ o' [chow]:**\n- Ch = 'ch' sound\n- Ã o = 'ow' sound\n- Thanh huyá»n (giá»ng xuá»‘ng nháº¹)\n\nğŸ’¡ **Thá»­ nÃ³i cháº­m:** 'Sin... chow'\n\nHÃ£y luyá»‡n táº­p 5 láº§n vÃ  cho tÃ´i biáº¿t cáº£m nháº­n!"}
        ]
    },
    {
        "conversation": [
            {"role": "student", "content": "TÃ´i muá»‘n há»c tá»« vá»±ng vá» gia Ä‘Ã¬nh"},
            {"role": "tutor", "content": "Gia Ä‘Ã¬nh lÃ  chá»§ Ä‘á» quan trá»ng! ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦\n\n**ThÃ nh viÃªn gia Ä‘Ã¬nh:**\nâ€¢ Ba/Cha = Father\nâ€¢ Máº¹/MÃ¡ = Mother\nâ€¢ Anh = Older brother\nâ€¢ Chá»‹ = Older sister  \nâ€¢ Em = Younger sibling\nâ€¢ Con = Child\nâ€¢ Ã”ng = Grandfather\nâ€¢ BÃ  = Grandmother\n\n**CÃ¢u máº«u:**\n- 'Gia Ä‘Ã¬nh tÃ´i cÃ³ 4 ngÆ°á»i' = My family has 4 people\n- 'Ba máº¹ tÃ´i ráº¥t tá»‘t' = My parents are very good\n\nBáº¡n thá»­ táº¡o cÃ¢u vá» gia Ä‘Ã¬nh mÃ¬nh xem?"}
        ]
    },
    {
        "conversation": [
            {"role": "student", "content": "6 thanh Ä‘iá»‡u tiáº¿ng Viá»‡t lÃ  gÃ¬?"},
            {"role": "tutor", "content": "ÄÃ¢y lÃ  Ä‘iá»u quan trá»ng nháº¥t trong tiáº¿ng Viá»‡t! ğŸµ\n\n**6 Thanh Ä‘iá»‡u vá»›i tá»« 'ma':**\n\n1ï¸âƒ£ **Ngang** (ma) - Ghost ğŸ‘»\n   - Giá»ng báº±ng, khÃ´ng lÃªn xuá»‘ng\n   \n2ï¸âƒ£ **Huyá»n** (mÃ ) - Which/That\n   - Giá»ng xuá»‘ng tá»« tá»« â†˜ï¸\n   \n3ï¸âƒ£ **Sáº¯c** (mÃ¡) - Mother/Cheek\n   - Giá»ng lÃªn cao â†—ï¸\n   \n4ï¸âƒ£ **Há»i** (máº£) - Tomb\n   - Giá»ng lÃªn rá»“i xuá»‘ng â†—ï¸â†˜ï¸\n   \n5ï¸âƒ£ **NgÃ£** (mÃ£) - Code/Horse\n   - Giá»ng gÃ£y, ngáº¯t quÃ£ng â†—ï¸â†˜ï¸â†—ï¸\n   \n6ï¸âƒ£ **Náº·ng** (máº¡) - Rice seedling\n   - Giá»ng xuá»‘ng ngáº¯n, dá»©t khoÃ¡t â¬‡ï¸\n\nğŸ¯ **Thá»±c hÃ nh:** HÃ£y Ä‘á»c to 6 tá»« nÃ y liÃªn tá»¥c!"}
        ]
    },
    {
        "conversation": [
            {"role": "student", "content": "Ká»ƒ cho tÃ´i vá» vÄƒn hÃ³a Viá»‡t Nam"},
            {"role": "tutor", "content": "VÄƒn hÃ³a Viá»‡t Nam ráº¥t phong phÃº vÃ  Ä‘á»™c Ä‘Ã¡o! ğŸ‡»ğŸ‡³\n\n**ğŸ Truyá»n thá»‘ng quan trá»ng:**\n\nâ€¢ **Táº¿t NguyÃªn ÄÃ¡n** - NÄƒm má»›i Ã‚m lá»‹ch\n  - Lá»… quan trá»ng nháº¥t trong nÄƒm\n  - Ä‚n bÃ¡nh chÆ°ng, má»©t Táº¿t\n  - LÃ¬ xÃ¬ Ä‘á» cho tráº» em\n\nâ€¢ **Ão dÃ i** - Trang phá»¥c truyá»n thá»‘ng\n  - Thanh lá»‹ch, duyÃªn dÃ¡ng\n  - Máº·c trong dá»‹p Ä‘áº·c biá»‡t\n\nâ€¢ **CÃºng tá»• tiÃªn** - Thá» cÃºng gia Ä‘Ã¬nh\n  - BÃ n thá» trong nhÃ \n  - TÃ´n kÃ­nh ngÆ°á»i cÃ³ cÃ´ng\n\n**ğŸœ áº¨m thá»±c Ä‘áº·c sáº¯c:**\nâ€¢ Phá»Ÿ - MÃ³n quá»‘c há»“n\nâ€¢ BÃ¡nh mÃ¬ - Sandwich Viá»‡t\nâ€¢ CÃ  phÃª sá»¯a Ä‘Ã¡ - Äá»“ uá»‘ng truyá»n thá»‘ng\n\nBáº¡n muá»‘n tÃ¬m hiá»ƒu sÃ¢u vá» khÃ­a cáº¡nh nÃ o?"}
        ]
    }
]

def create_training_dataset():
    """Táº¡o dataset cho fine-tuning"""
    formatted_data = []
    
    for item in TRAINING_DATA:
        conversation = item["conversation"]
        
        # Format theo style PhoGPT
        text = ""
        for turn in conversation:
            if turn["role"] == "student":
                text += f"### Há»c viÃªn: {turn['content']}\n\n"
            else:
                text += f"### Gia sÆ°: {turn['content']}\n\n"
        
        formatted_data.append({"text": text})
    
    return Dataset.from_list(formatted_data)

def fine_tune_phogpt():
    """Fine-tune PhoGPT cho Vietnamese tutoring"""
    
    print("ğŸš€ Báº¯t Ä‘áº§u fine-tuning PhoGPT...")
    
    # Load tokenizer vÃ  model
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        cache_dir=CACHE_DIR,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        cache_dir=CACHE_DIR,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    
    # ThÃªm pad token náº¿u chÆ°a cÃ³
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Táº¡o dataset
    dataset = create_training_dataset()
    
    def tokenize_function(examples):
        texts = examples["text"]
        # Flatten náº¿u cÃ³ list lá»“ng nhau (phÃ²ng trÆ°á»ng há»£p lá»—i dataset)
        flat_texts = []
        for t in texts:
            if isinstance(t, list):
                flat_texts.extend([str(x) for x in t])
            else:
                flat_texts.append(str(t))
        print("DEBUG - Sample texts:", flat_texts[:2])
        return tokenizer(
            flat_texts,
            truncation=True,
            padding=True,
            max_length=1024
        )
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM, not masked LM
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=1,  # Small batch for memory
        gradient_accumulation_steps=4,
        warmup_steps=50,
        logging_steps=10,
        save_steps=100,
        evaluation_strategy="no",
        save_total_limit=2,
        prediction_loss_only=True,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        learning_rate=5e-5,
        weight_decay=0.01,
        fp16=torch.cuda.is_available(),  # Use fp16 if GPU available
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )
    
    # Start training
    print("ğŸ“š Báº¯t Ä‘áº§u training...")
    trainer.train()
    
    # Save model
    print("ğŸ’¾ LÆ°u model...")
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"âœ… Fine-tuning hoÃ n táº¥t! Model Ä‘Æ°á»£c lÆ°u táº¡i: {OUTPUT_DIR}")

if __name__ == "__main__":
    fine_tune_phogpt()