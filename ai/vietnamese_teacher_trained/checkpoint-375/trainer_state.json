{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.869565217391305,
  "eval_steps": 500,
  "global_step": 375,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.2898550724637681,
      "grad_norm": 4.624680042266846,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.5056,
      "step": 5
    },
    {
      "epoch": 0.5797101449275363,
      "grad_norm": 4.444783687591553,
      "learning_rate": 9e-06,
      "loss": 3.5055,
      "step": 10
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 3.8973777294158936,
      "learning_rate": 1.4e-05,
      "loss": 3.364,
      "step": 15
    },
    {
      "epoch": 1.1159420289855073,
      "grad_norm": 3.1912965774536133,
      "learning_rate": 1.9e-05,
      "loss": 3.2548,
      "step": 20
    },
    {
      "epoch": 1.4057971014492754,
      "grad_norm": 2.812528371810913,
      "learning_rate": 1.9813953488372094e-05,
      "loss": 3.166,
      "step": 25
    },
    {
      "epoch": 1.6956521739130435,
      "grad_norm": 2.99674916267395,
      "learning_rate": 1.9581395348837212e-05,
      "loss": 2.9644,
      "step": 30
    },
    {
      "epoch": 1.9855072463768115,
      "grad_norm": 2.910062551498413,
      "learning_rate": 1.9348837209302327e-05,
      "loss": 2.7593,
      "step": 35
    },
    {
      "epoch": 2.2318840579710146,
      "grad_norm": 2.8731765747070312,
      "learning_rate": 1.9116279069767442e-05,
      "loss": 2.7976,
      "step": 40
    },
    {
      "epoch": 2.5217391304347827,
      "grad_norm": 2.603597640991211,
      "learning_rate": 1.888372093023256e-05,
      "loss": 2.6321,
      "step": 45
    },
    {
      "epoch": 2.8115942028985508,
      "grad_norm": 2.5853664875030518,
      "learning_rate": 1.865116279069768e-05,
      "loss": 2.602,
      "step": 50
    },
    {
      "epoch": 3.0579710144927534,
      "grad_norm": 2.4910848140716553,
      "learning_rate": 1.8418604651162793e-05,
      "loss": 2.3792,
      "step": 55
    },
    {
      "epoch": 3.3478260869565215,
      "grad_norm": 2.655578136444092,
      "learning_rate": 1.8186046511627908e-05,
      "loss": 2.4464,
      "step": 60
    },
    {
      "epoch": 3.63768115942029,
      "grad_norm": 2.7290544509887695,
      "learning_rate": 1.7953488372093023e-05,
      "loss": 2.2895,
      "step": 65
    },
    {
      "epoch": 3.927536231884058,
      "grad_norm": 3.09889554977417,
      "learning_rate": 1.772093023255814e-05,
      "loss": 2.2976,
      "step": 70
    },
    {
      "epoch": 4.173913043478261,
      "grad_norm": 2.7790379524230957,
      "learning_rate": 1.748837209302326e-05,
      "loss": 2.1731,
      "step": 75
    },
    {
      "epoch": 4.463768115942029,
      "grad_norm": 2.365140199661255,
      "learning_rate": 1.7255813953488374e-05,
      "loss": 2.0662,
      "step": 80
    },
    {
      "epoch": 4.753623188405797,
      "grad_norm": 2.74302077293396,
      "learning_rate": 1.702325581395349e-05,
      "loss": 2.1609,
      "step": 85
    },
    {
      "epoch": 5.0,
      "grad_norm": 4.161542892456055,
      "learning_rate": 1.6790697674418607e-05,
      "loss": 2.2497,
      "step": 90
    },
    {
      "epoch": 5.2898550724637685,
      "grad_norm": 2.3601982593536377,
      "learning_rate": 1.6558139534883722e-05,
      "loss": 2.1743,
      "step": 95
    },
    {
      "epoch": 5.579710144927536,
      "grad_norm": 2.5023691654205322,
      "learning_rate": 1.632558139534884e-05,
      "loss": 1.9479,
      "step": 100
    },
    {
      "epoch": 5.869565217391305,
      "grad_norm": 2.0720114707946777,
      "learning_rate": 1.6093023255813955e-05,
      "loss": 1.9372,
      "step": 105
    },
    {
      "epoch": 6.115942028985507,
      "grad_norm": 1.8348793983459473,
      "learning_rate": 1.5860465116279073e-05,
      "loss": 1.9267,
      "step": 110
    },
    {
      "epoch": 6.405797101449275,
      "grad_norm": 2.138099193572998,
      "learning_rate": 1.5627906976744188e-05,
      "loss": 1.9441,
      "step": 115
    },
    {
      "epoch": 6.695652173913043,
      "grad_norm": 2.3740923404693604,
      "learning_rate": 1.5395348837209303e-05,
      "loss": 1.8897,
      "step": 120
    },
    {
      "epoch": 6.9855072463768115,
      "grad_norm": 2.176391363143921,
      "learning_rate": 1.5162790697674419e-05,
      "loss": 1.9107,
      "step": 125
    },
    {
      "epoch": 7.231884057971015,
      "grad_norm": 2.1135451793670654,
      "learning_rate": 1.4930232558139537e-05,
      "loss": 1.8202,
      "step": 130
    },
    {
      "epoch": 7.521739130434782,
      "grad_norm": 1.9172354936599731,
      "learning_rate": 1.4697674418604652e-05,
      "loss": 1.8729,
      "step": 135
    },
    {
      "epoch": 7.811594202898551,
      "grad_norm": 2.0103394985198975,
      "learning_rate": 1.4465116279069768e-05,
      "loss": 1.7532,
      "step": 140
    },
    {
      "epoch": 8.057971014492754,
      "grad_norm": 1.9279977083206177,
      "learning_rate": 1.4232558139534885e-05,
      "loss": 1.8599,
      "step": 145
    },
    {
      "epoch": 8.347826086956522,
      "grad_norm": 2.0445380210876465,
      "learning_rate": 1.4e-05,
      "loss": 1.6782,
      "step": 150
    },
    {
      "epoch": 8.63768115942029,
      "grad_norm": 2.4924676418304443,
      "learning_rate": 1.3767441860465118e-05,
      "loss": 1.7737,
      "step": 155
    },
    {
      "epoch": 8.927536231884059,
      "grad_norm": 1.9849685430526733,
      "learning_rate": 1.3534883720930234e-05,
      "loss": 1.7621,
      "step": 160
    },
    {
      "epoch": 9.173913043478262,
      "grad_norm": 2.3404226303100586,
      "learning_rate": 1.330232558139535e-05,
      "loss": 1.6526,
      "step": 165
    },
    {
      "epoch": 9.46376811594203,
      "grad_norm": 2.339937686920166,
      "learning_rate": 1.3069767441860466e-05,
      "loss": 1.7386,
      "step": 170
    },
    {
      "epoch": 9.753623188405797,
      "grad_norm": 2.2452127933502197,
      "learning_rate": 1.2837209302325582e-05,
      "loss": 1.6446,
      "step": 175
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.967428207397461,
      "learning_rate": 1.26046511627907e-05,
      "loss": 1.6134,
      "step": 180
    },
    {
      "epoch": 10.289855072463768,
      "grad_norm": 2.503978729248047,
      "learning_rate": 1.2372093023255815e-05,
      "loss": 1.6125,
      "step": 185
    },
    {
      "epoch": 10.579710144927537,
      "grad_norm": 2.0870237350463867,
      "learning_rate": 1.2139534883720932e-05,
      "loss": 1.6894,
      "step": 190
    },
    {
      "epoch": 10.869565217391305,
      "grad_norm": 2.850616931915283,
      "learning_rate": 1.1906976744186047e-05,
      "loss": 1.5857,
      "step": 195
    },
    {
      "epoch": 11.115942028985508,
      "grad_norm": 2.107088088989258,
      "learning_rate": 1.1674418604651163e-05,
      "loss": 1.4694,
      "step": 200
    },
    {
      "epoch": 11.405797101449275,
      "grad_norm": 3.74662446975708,
      "learning_rate": 1.144186046511628e-05,
      "loss": 1.5506,
      "step": 205
    },
    {
      "epoch": 11.695652173913043,
      "grad_norm": 2.0961227416992188,
      "learning_rate": 1.1209302325581398e-05,
      "loss": 1.6041,
      "step": 210
    },
    {
      "epoch": 11.985507246376812,
      "grad_norm": 2.776426076889038,
      "learning_rate": 1.0976744186046513e-05,
      "loss": 1.5488,
      "step": 215
    },
    {
      "epoch": 12.231884057971014,
      "grad_norm": 2.537397861480713,
      "learning_rate": 1.0744186046511629e-05,
      "loss": 1.5147,
      "step": 220
    },
    {
      "epoch": 12.521739130434783,
      "grad_norm": 2.144063711166382,
      "learning_rate": 1.0511627906976744e-05,
      "loss": 1.5763,
      "step": 225
    },
    {
      "epoch": 12.81159420289855,
      "grad_norm": 3.5729165077209473,
      "learning_rate": 1.027906976744186e-05,
      "loss": 1.4823,
      "step": 230
    },
    {
      "epoch": 13.057971014492754,
      "grad_norm": 2.5203187465667725,
      "learning_rate": 1.0046511627906979e-05,
      "loss": 1.5101,
      "step": 235
    },
    {
      "epoch": 13.347826086956522,
      "grad_norm": 1.9326938390731812,
      "learning_rate": 9.813953488372093e-06,
      "loss": 1.4997,
      "step": 240
    },
    {
      "epoch": 13.63768115942029,
      "grad_norm": 2.4989752769470215,
      "learning_rate": 9.58139534883721e-06,
      "loss": 1.4826,
      "step": 245
    },
    {
      "epoch": 13.927536231884059,
      "grad_norm": 2.973724603652954,
      "learning_rate": 9.348837209302326e-06,
      "loss": 1.4651,
      "step": 250
    },
    {
      "epoch": 14.173913043478262,
      "grad_norm": 2.072314500808716,
      "learning_rate": 9.116279069767443e-06,
      "loss": 1.3625,
      "step": 255
    },
    {
      "epoch": 14.46376811594203,
      "grad_norm": 2.300560712814331,
      "learning_rate": 8.88372093023256e-06,
      "loss": 1.5136,
      "step": 260
    },
    {
      "epoch": 14.753623188405797,
      "grad_norm": 2.031214952468872,
      "learning_rate": 8.651162790697674e-06,
      "loss": 1.374,
      "step": 265
    },
    {
      "epoch": 15.0,
      "grad_norm": 10.508902549743652,
      "learning_rate": 8.418604651162792e-06,
      "loss": 1.4386,
      "step": 270
    },
    {
      "epoch": 15.289855072463768,
      "grad_norm": 2.360222816467285,
      "learning_rate": 8.186046511627907e-06,
      "loss": 1.3951,
      "step": 275
    },
    {
      "epoch": 15.579710144927537,
      "grad_norm": 2.175689935684204,
      "learning_rate": 7.953488372093024e-06,
      "loss": 1.3483,
      "step": 280
    },
    {
      "epoch": 15.869565217391305,
      "grad_norm": 2.627835512161255,
      "learning_rate": 7.72093023255814e-06,
      "loss": 1.5357,
      "step": 285
    },
    {
      "epoch": 16.115942028985508,
      "grad_norm": 2.448745012283325,
      "learning_rate": 7.488372093023256e-06,
      "loss": 1.4339,
      "step": 290
    },
    {
      "epoch": 16.405797101449274,
      "grad_norm": 2.101996660232544,
      "learning_rate": 7.255813953488373e-06,
      "loss": 1.3657,
      "step": 295
    },
    {
      "epoch": 16.695652173913043,
      "grad_norm": 3.1498775482177734,
      "learning_rate": 7.023255813953489e-06,
      "loss": 1.3972,
      "step": 300
    },
    {
      "epoch": 16.985507246376812,
      "grad_norm": 2.392395257949829,
      "learning_rate": 6.790697674418605e-06,
      "loss": 1.408,
      "step": 305
    },
    {
      "epoch": 17.231884057971016,
      "grad_norm": 2.4262681007385254,
      "learning_rate": 6.558139534883722e-06,
      "loss": 1.3809,
      "step": 310
    },
    {
      "epoch": 17.52173913043478,
      "grad_norm": 2.6493122577667236,
      "learning_rate": 6.325581395348837e-06,
      "loss": 1.311,
      "step": 315
    },
    {
      "epoch": 17.81159420289855,
      "grad_norm": 2.610417604446411,
      "learning_rate": 6.093023255813954e-06,
      "loss": 1.47,
      "step": 320
    },
    {
      "epoch": 18.057971014492754,
      "grad_norm": 2.4977967739105225,
      "learning_rate": 5.86046511627907e-06,
      "loss": 1.3714,
      "step": 325
    },
    {
      "epoch": 18.347826086956523,
      "grad_norm": 2.147585868835449,
      "learning_rate": 5.627906976744186e-06,
      "loss": 1.3623,
      "step": 330
    },
    {
      "epoch": 18.63768115942029,
      "grad_norm": 2.1748876571655273,
      "learning_rate": 5.395348837209303e-06,
      "loss": 1.3568,
      "step": 335
    },
    {
      "epoch": 18.92753623188406,
      "grad_norm": 2.3906760215759277,
      "learning_rate": 5.162790697674419e-06,
      "loss": 1.36,
      "step": 340
    },
    {
      "epoch": 19.17391304347826,
      "grad_norm": 2.2029032707214355,
      "learning_rate": 4.9302325581395355e-06,
      "loss": 1.326,
      "step": 345
    },
    {
      "epoch": 19.463768115942027,
      "grad_norm": 3.2001566886901855,
      "learning_rate": 4.697674418604651e-06,
      "loss": 1.3494,
      "step": 350
    },
    {
      "epoch": 19.753623188405797,
      "grad_norm": 2.7909743785858154,
      "learning_rate": 4.465116279069768e-06,
      "loss": 1.2938,
      "step": 355
    },
    {
      "epoch": 20.0,
      "grad_norm": 4.511092662811279,
      "learning_rate": 4.232558139534884e-06,
      "loss": 1.331,
      "step": 360
    },
    {
      "epoch": 20.28985507246377,
      "grad_norm": 2.495225191116333,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.3122,
      "step": 365
    },
    {
      "epoch": 20.579710144927535,
      "grad_norm": 2.1983437538146973,
      "learning_rate": 3.7674418604651163e-06,
      "loss": 1.3213,
      "step": 370
    },
    {
      "epoch": 20.869565217391305,
      "grad_norm": 2.4110522270202637,
      "learning_rate": 3.534883720930233e-06,
      "loss": 1.3026,
      "step": 375
    }
  ],
  "logging_steps": 5,
  "max_steps": 450,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 25,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 376260526080000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
